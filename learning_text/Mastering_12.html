<html>
<head>
  <title>Evernote Export</title>
  <basefont face="等线" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/307027 (zh-CN, DDL); Windows/10.0.0 (Win64);"/>
  <style>
    body, td {
      font-family: 等线;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="713"/>

<div>
<span><div>From the Perceptron to Artificial Neural Networks 从感知器到人工神经网络</div><div><br/></div><div>我们学习的感知器不是通用函数近似，它的决策边界必须是超平面。在本章中我们将讨论ANN，对监督和非监督任务来说是有力的非线性模型，它使用不同的策略来战胜感知器的限制。如果感知器近似于一个神经元，那么ANN就是神经网，近似于大脑。因为几十亿的神经元、几万亿的突触组成了人类的大脑，ANN是直接的关于人工神经元的图。这个图的边缘是有权重的，这些权重是模型必须学习的参数。</div><div><br/></div><div>本章将提供小的，喂养导向的人工神经网络训练和结构概述。sklearn操作神经网络为分类、回归和特征提取。然而，这些操作仅适用于小的网络。训练神经网络是非常消耗计算能力的；事实上大多数使用有几千个并行处理核的图形处理器训练神经网络。sklearn不支持GPU，并且在近来也不可能支持。GPU加速还不非常成熟，但是很值得探索。sklearn支持它将增加很多依赖，并且和简单安装在很多平台上的目标是冲突的。而且其它的机器学习算法很少需要GPU加速在和神经网络相似的范围里。训练神经网络是更好服务的，在目标建立库例如Caffe，TensorFlow，和Keras，而不是一般的目标机器学习库例如sklearn。</div><div><br/></div><div>当我们为对象识别不使用sklearn去训练深度卷积神经网络或者为演讲识别、理解工作不使用回归神经网络是任务非常重要的先决条件。</div><div><br/></div><div>page 336</div><div>非线性决策边界</div><div>XOR：二者中仅有一个为真。</div><div>NAND：不能全为真，也就是not and。</div><div>不是试着用一个单一感知器表示XOR，我们将建立一个ANN从多元人工神经网络每一个近似一个线性函数。每一个实例的特征表示将被输入两个神经元，一个将代表NAND，另一个代表OR。输出这些将被第三个代表AND的神经元接受去检测所有的XOR为真。</div><div><br/></div><div>正向反馈和ANN反馈</div><div>ANN用三个组成部分描述。首先是模型的建筑，或者拓扑结构：描述了神经元的类型和它们之间练习的结构。然后我们有人工神经元动作函数。第三个组成部分是学习算法发现最优的权重值。</div><div>有两个主要的ANN类型。正向反馈神经网络是最普通的类型并且被定义通过它们的直接非循环的图形。在正向反馈中信息仅在一个方向传输，传递到输出层。相反的，反馈神经网络，或者回归神经网络，包括循环。反馈循环能代表导致以输入为基础的网络行为随时间改变的网络的内部状态。正向反馈神经网络通常学习一个函数去映射输入到输出。例如正向反馈网能被用来识别图片中的对象或者预测软件即服务的产品的订阅者的相似度。正向反馈神经网络暂存的行为让他们适合于程序序列输入。正向反馈神经网络曾被用于翻译文档或者自动化处理演讲的字幕。因为反馈神经网络不能在sklearn中操作，我们将限制本次讨论于正向反馈神经网络中。</div><div><br/></div><div>page 341</div><div>多层感知器</div><div>多层感知器是简单的ANN，它的名字，然而，是用词不当的。多层感知器不是一个感知器有很多层，而是很多类似感知器的多层的人工神经网。多层感知器有三层或者更多层人工神经形成直接的、非循环的图表。一般的，每一层是全连接的到下一层，输出层或者称之为动能函数，每一人工神经网对下一层来说都是输入层。特征是输入穿过输入层。在输入层中最简单的神经网被连接至最少一层隐藏层。隐藏层表示使变迟变量；这些不能在训练集中被观测到。在这些层中隐藏的神经网也被称之为隐藏单元。最终最后的隐藏层是连接到输出层的，也是反馈型变量预测的值。</div><div>回顾第十章，感知器的感知有一或多二进制输入，一个二进制输出，并且海微赛德步骤动能函数。一个小的感知器权重的改变也许在输入中没有任何影响，或者它可能引起输出层从一到零的轻抛。由于我们改变了它的权重，这将是理解网络表现如何改变变得困难。因为这个原因，我们将从不同神经元类型建立MLP。一个乙字状神经元有一或者多实数值得输入和一个实数输出，并且它使用乙字状动能函数。</div><div><br/></div><div>page 347</div><div>反向传播算法</div><div>我们已学过等降梯度通过计算梯度值和使用梯度更新函数参数迭代最小化函数。为了多层感知器最小化费用函数，我们需要计算它梯度的能力。回顾多层感知器包含表示最迟变量单元的层。我们不能使用费用函数计算它们的误差；训练集指示整个网络应得的输出，但是它不描述隐藏单元应有行为。自从我们不能计算隐藏单元的误差，我们不能计算它们的梯度或更新它们的权重。一种朴素的解决整个问题的方法是随机改变隐藏单元的权重。如果随机改变权重中的一个降低了费用函数的值，那么权重被更新，其它的改变被评价。这种解决问题的计算消耗在不重要的网络中是被禁止的。在这方面，我们将描述一个更有效的解决方式，我们将使用反向传播算法计算有尊重权重神经网络费用函数的梯度，或者后撑。反向传播算法允许我们理解每一个权重如何影响误差，并且如何更新权重去最小化费用函数。</div><div>算法的全名是复合反向传播算法，并且在计算梯度时在流过网络层认为去哪个方向的误差。后撑普遍用在联合最小化算法中，就像去训练正向反馈神经网络的梯度下降。从理论上讲它能被用来在任何数量的层数、任何数量的隐藏单元情况下去训练正向反馈网络。</div><div>就像梯度下降，支撑是可迭代的算法每一次迭代包含两个阶段。第一个阶段是向前反向传播，或者向前穿过。损失函数能被用来计算预测的误差。第二个阶段是反向船舶算法，误差从损失函数到输入层被反向传播，所以每个神经元贡献的误差能被近似得出。这个程序依赖于链式规则，这个能被用来计算两个或更多函数组成的衍生物。我们在之前已经演示神经网络能由线性函数组成去近似复杂的非线性函数。这些误差能被用来计算梯度梯度下降需要更新权重。权重更新之后，特征能被反向传播通过网络开始下一个迭代。</div><div>反向传播通过网络，我们计算神经元的动能在一个层，并且输入动能连接到神经元在下一层。为了这样我们必须首先计算在层中每个神经元的预动能。回顾神经元的预动能是输入和权重的联合。下一步我们通过应用它的动能函数到它的预动能计算每个神经元的动能。动能是在下层网络中的输入。</div><div>反向传播通过网络，我们首先用尊重最后隐藏层的动能计算费用函数的部分衍生物。我们能用尊重它的预动能计算最后掩藏层动能部分衍生物。下一步我们用尊重它的权重计算部分最后隐藏层的预动能衍生物，等到我们都抵达输入层。通过这个程序，我们近似每个神经元的误差贡献，我们计算梯度需要去更新它们的权重和最小化费用函数。更准确的是在每一层的每一单元，我们必须计算两个部分衍生物。第一个尊重动能单元的误差衍生物。这个衍生物没有被用来更新单元的权重，而不是它使用更新单元的权重连接到之前的层。第二我们将用尊重单元的权重为了更新它们的值和最少的费用函数计算部分衍生物。让我们工作通过一个案例。我们将用两个输入单元训练一个神经网络，一个掩藏层用两个隐藏单元，一个是输出单元。</div><div><br/></div><div>page 355</div><div>我将分叉另外的进程在交叉验证时，需要执行从一个主保护块。攀登特征是特别重要的对ANN，并且将帮助一些学习算法去集中更快。</div></span>
</div></body></html> 